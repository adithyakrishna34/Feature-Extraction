{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFPUsfZ9l1lt",
        "outputId": "aa989dba-9a3c-452e-f27d-c4a5bfea12dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=22f3931c1a8b3f124a3636f4ef22c9ae294dc4b7c4bbf816b7f6e6bf48fb8604\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer,OneHotEncoder,StandardScaler,VectorAssembler,ChiSqSelector\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "AjoD7IGimXVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a spark session\n",
        "spark=SparkSession.builder.appName('BostonHousingFeatureEngineeringDone').getOrCreate()"
      ],
      "metadata": {
        "id": "uvtHUHzXmXZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the datasetinto Pyspark\n",
        "#Assume we have a csv file with headers\n",
        "data=spark.read.csv('/content/HousingData.csv',header=True,inferSchema=True)\n",
        "\n",
        "# Print the schema to see data types\n",
        "data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhfoWI9kmXcU",
        "outputId": "58439e8a-d8d9-4173-df85-f4daf33e50cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CRIM: string (nullable = true)\n",
            " |-- ZN: string (nullable = true)\n",
            " |-- INDUS: string (nullable = true)\n",
            " |-- CHAS: string (nullable = true)\n",
            " |-- NOX: double (nullable = true)\n",
            " |-- RM: double (nullable = true)\n",
            " |-- AGE: string (nullable = true)\n",
            " |-- DIS: double (nullable = true)\n",
            " |-- RAD: integer (nullable = true)\n",
            " |-- TAX: integer (nullable = true)\n",
            " |-- PTRATIO: double (nullable = true)\n",
            " |-- B: double (nullable = true)\n",
            " |-- LSTAT: string (nullable = true)\n",
            " |-- MEDV: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categorical and numerical columns\n",
        "categorical_cols=['CHAS','RAD']\n",
        "numerical_cols=['CRIM','ZN','INDUS','NOX','RM','AGE','DIS','TAX','PTRATIO','B','LSTAT']"
      ],
      "metadata": {
        "id": "AODftlQomXe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import DoubleType\n",
        "from pyspark.sql.types import DoubleType"
      ],
      "metadata": {
        "id": "3J1KCRbLmXhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer"
      ],
      "metadata": {
        "id": "ocKKzLewq7lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Handle missing values and convert to appropriate types\n",
        "for col_name in numerical_cols + ['MEDV']:\n",
        "  data = data.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
        "\n",
        "# Use imputer to handle Nan values in numerical columns\n",
        "imputer =Imputer(inputCols=numerical_cols,outputCols=numerical_cols)\n",
        "imputer.setStrategy('mean')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDYJoaZTmXnL",
        "outputId": "bd14c1b2-7eba-4d3b-ba07-027fece1cff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Imputer_f53cb4ed24e4"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. handle categorical features\n",
        "# For categorical columns, replace NaN with a placeholder value before indexing\n",
        "from pyspark.sql.functions import when,col\n",
        "for cat_col in categorical_cols:\n",
        "  data=data.withColumn(cat_col,when(col(cat_col).isNull(),'Unknown').otherwise(col(cat_col)))\n",
        "indexers=[StringIndexer(inputCol=col,outputCol=f'{col}_indexed',handleInvalid='keep')for col in categorical_cols]\n",
        "encoders=[OneHotEncoder(inputCol=f'{col}_indexed',outputCol=f'{col}_encoded')for col in categorical_cols ]"
      ],
      "metadata": {
        "id": "kodqqOfsmXqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. scale numerical features\n",
        "assembler_num=VectorAssembler(inputCols=numerical_cols,outputCol='num_features')\n",
        "scaler=StandardScaler(inputCol='num_features',outputCol='scaled_num_features',withStd=True,withMean=True)"
      ],
      "metadata": {
        "id": "mowfnR3AmXtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 . Combine features\n",
        "encoded_cols=[f'{col}_encoded'for col in categorical_cols]\n",
        "assembler=VectorAssembler(inputCols=encoded_cols + ['scaled_num_features'],outputCol='features')"
      ],
      "metadata": {
        "id": "kxVgpCsumXyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Perform feature selection\n",
        "median_medv=data.approxQuantile('MEDV',[0.5],0.01)[0]\n",
        "data=data.withColumn('label',(col('MEDV')>median_medv).cast('double'))\n",
        "selector=ChiSqSelector(numTopFeatures=10,featuresCol='features',outputCol='selected_features',labelCol='label')"
      ],
      "metadata": {
        "id": "VUm3m8a7mX1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the pipeline\n",
        "pipeline=Pipeline(stages=[imputer]+indexers+encoders+[assembler_num,scaler,assembler,selector])\n",
        "try:\n",
        "  model=pipeline.fit(data)\n",
        "  result=model.transform(data)\n",
        "\n",
        "  # Shoe the result\n",
        "  result.select('selected_features','label').show(5,truncate=False)\n",
        "\n",
        "  # get feature importances\n",
        "  feature_importances=model.stages[-1].selectedFeatures\n",
        "  print('Selected feature indices:',feature_importances)\n",
        "except Exception as e:\n",
        "  print(f'An error occured: {str(e)}')\n",
        "  # optionally , you can add more detailed error handling here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEGHIfLrmX60",
        "outputId": "4677e649-580d-4813-b351-39be6391e3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|selected_features                                                                                                          |label|\n",
            "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|(10,[5,6,7,8,9],[0.29614984653077786,-1.3097142836688263,-0.14407485473245793,-0.6659491794887344,-1.4575579672895913])    |1.0  |\n",
            "|(10,[5,6,7,8,9],[-0.4891544449063988,-0.5991779446787058,-0.7395303607434325,-0.9863533804386955,-0.3027944997494501])     |1.0  |\n",
            "|(10,[5,6,7,8,9],[-0.4891544449063988,-0.5991779446787058,-0.7395303607434325,-0.9863533804386955,-0.3027944997494501])     |1.0  |\n",
            "|(10,[3,5,6,7,8,9],[1.0,-0.4891544449063988,-1.3291196878849432,-0.8344580501075004,-1.105021603012755,0.11292034856500006])|1.0  |\n",
            "|(10,[3,5,6,7,8,9],[1.0,-0.4891544449063988,-1.3291196878849432,-0.8344580501075004,-1.105021603012755,0.11292034856500006])|1.0  |\n",
            "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Selected feature indices: [1, 3, 4, 6, 9, 13, 14, 15, 19, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "Vt_LfknmwI6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3YHdoMn_mX98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvHuF790mYAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}